@inproceedings{11084319,
 abstract = {Masked image modeling (MIM), which is a self-supervised learning method in computer vision, excels in image-and video-level recognition tasks by providing robust and generalized feature representations. However, most MIM methods incorporate plain Vision Transformers (ViTs), which lack the capability to produce multiscale features, thereby limiting their effectiveness in more complex object-level recognition tasks. Extracting multiscale hierarchical features using a convolutional stem and fully fusing local and global information within all feature representations are crucial for applying the MIM framework to object-level recognition. To address this issue, we propose an effective multiscale feature extraction mechanism that integrates local and global dependencies from the convolutional stem and ViT within the MIM framework. Our method was evaluated on object detection and instance segmentation tasks using the MS COCO dataset. It exhibits superior performance by effectively fusing local and global information across all feature scales, achieving comparable results to those of state-of-the-art methods while using 25% fewer training samples.},
 author = {Amarbayasgalan, Tsatsral and Wang, Sungjun and Kim, Mooseop and Jeong, Chi Yoon},
 booktitle = {2025 IEEE International Conference on Image Processing (ICIP)},
 date = {2025-09-14},
 doi = {10.1109/ICIP55913.2025.11084319},
 issn = {2381-8549},
 keywords = {Training;Computer vision;Image recognition;Limiting;Autoencoders;Object detection;Self-supervised learning;Predictive models;Feature extraction;Transformers;Masked image modeling;Masked autoencoder;Vision Transformer;Dense prediction},
 month = {Sep.},
 number = {},
 pages = {677-682},
 title = {Enhancing Multiscale Feature Representation For Object-Level Recognition In Masked Image Modeling},
 volume = {},
 year = {2025}
}
