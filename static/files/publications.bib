@ARTICLE{25-TNSRE-moon,
  author={Deok Moon, Kyeong and Kyung Park, Yun and Seop Kim, Moo and Jeong, Chi Yoon},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Improving Acceptance to Sensory Substitution: A Study on the V2A-SS Learning Model Based on Information Processing Learning Theory}, 
  year={2025},
  volume={33},
  number={},
  pages={1097-1107},
  abstract={The visual sensory organ (VSO) serves as the primary channel for transmitting external information to the brain; therefore, damage to the VSO can severely limit daily activities. Visual-to-Auditory Sensory Substitution (V2A-SS), an innovative approach to restoring vision, offers a promising solution by leveraging neuroplasticity to convey visual information via auditory channels. Advances in information technology and artificial intelligence mitigate technical challenges such as low resolution and limited bandwidth, thereby enabling broader applicability of V2A-SS. Despite these advances, integrating V2A-SS effectively into everyday life necessitates extensive training and adaptation. Therefore, alongside addressing technical challenges, investigating effective learning strategies to accelerate the acceptance of V2A-SS is crucial. This study introduces a V2A-SS learning model based on the Information Processing Learning Theory (IPLT), encompassing the stages of “concept acquisition, rehearsal, assessment” to reduce the learning curve and enhance adaptation. The experimental results show that the proposed learning model improves recognition rates, achieving an 11% increase over simple random repetition learning. This improvement is significantly higher than the gain of 2.72% achieved by optimizing the V2A-SS algorithm with Mel-Scaled Frequency Mapping. This study suggests that a structured learning model for sensory substitution technologies can contribute to bridging gaps between technical feasibility and practical application. This underscores the need to develop effective learning models, alongside technological advancements, to accelerate the adoption of V2A-SS and neuroplasticity.},
  keywords={Visualization;Training;Brain modeling;Neuroplasticity;Biological system modeling;Adaptation models;Image coding;Visual impairment;Auditory system;Sensor systems;Learning model for sensory substitution;neuroplasticity;vision-to-auditory sensory substitution},
  doi={10.1109/TNSRE.2025.3548942},
  ISSN={1558-0210},
  month={March},
  date={2025-03-06}
  }

@INPROCEEDINGS{25-ICIP-Amarbayasgalan,
  author={Amarbayasgalan, Tsatsral and Wang, Sungjun and Kim, Mooseop and Jeong, Chi Yoon},
  booktitle={2025 IEEE International Conference on Image Processing (ICIP)}, 
  title={Enhancing Multiscale Feature Representation For Object-Level Recognition In Masked Image Modeling}, 
  year={2025},
  volume={},
  number={},
  pages={677-682},
  abstract={Masked image modeling (MIM), which is a self-supervised learning method in computer vision, excels in image-and video-level recognition tasks by providing robust and generalized feature representations. However, most MIM methods incorporate plain Vision Transformers (ViTs), which lack the capability to produce multiscale features, thereby limiting their effectiveness in more complex object-level recognition tasks. Extracting multiscale hierarchical features using a convolutional stem and fully fusing local and global information within all feature representations are crucial for applying the MIM framework to object-level recognition. To address this issue, we propose an effective multiscale feature extraction mechanism that integrates local and global dependencies from the convolutional stem and ViT within the MIM framework. Our method was evaluated on object detection and instance segmentation tasks using the MS COCO dataset. It exhibits superior performance by effectively fusing local and global information across all feature scales, achieving comparable results to those of state-of-the-art methods while using 25% fewer training samples.},
  keywords={Training;Computer vision;Image recognition;Limiting;Autoencoders;Object detection;Self-supervised learning;Predictive models;Feature extraction;Transformers;Masked image modeling;Masked autoencoder;Vision Transformer;Dense prediction},
  doi={10.1109/ICIP55913.2025.11084319},
  ISSN={2381-8549},
  month={Sep.},
  date={2025-09-14}
}

@InProceedings{24-ICPR-Amarbayasgalan,
author="Amarbayasgalan, Tsatsral
and Kim, Mooseop
and Jeong, Chi Yoon",
editor="Antonacopoulos, Apostolos
and Chaudhuri, Subhasis
and Chellappa, Rama
and Liu, Cheng-Lin
and Bhattacharya, Saumik
and Pal, Umapada",
title="An Improved YOLOF for Scale Imbalance with Dilated Attention",
booktitle="Pattern Recognition",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="156--172",
abstract="Scale imbalance, where objects of different sizes are not equally represented in a dataset, is a common problem in real-world object detection scenarios that leads to significant performance degradation of object detection methods. Although several solutions have been proposed based on multilevel feature maps, these methods may not be suitable in real-time applications owing to their low speed and memory consumption. Recently, you only look one-level feature (YOLOF) was proposed based on a single-in-single-out (SiSo) architecture; the SiSo architecture is well suited for real-time applications with performance comparable to that of methods based on multilevel feature maps. However, they show limited performance when applied to real-world object detection scenarios with scale imbalance problems. Therefore, we propose a lightweight object detection method that can handle the scale imbalance problem while retaining the advantages of the SiSo framework. To mitigate the scale imbalance, we use dilated attention to extend the SiSo architecture and learn the scale range of objects. Extensive experiments on public datasets show the effectiveness of a dilated attention-based proposed method in scale-imbalanced scenarios. Our method achieves results comparable to those of the original YOLOF on the MS COCO and PASCAL VOC datasets. In particular, for imbalanced datasets, the proposed method outperforms the original YOLOF by 4.78{\%} on the first-person-walking-livingroom dataset and by 1.38{\%} on the imbalanced PASCAL VOC dataset in terms of average precision (AP)50.",
isbn="978-3-031-78447-7",
date="2024-12-03",
doi="https://doi.org/10.1007/978-3-031-78447-7_11"
}

@ARTICLE{24-ACCESS-Kim,
  author={Kim, Mooseop and Park, Yunkyung and Moon, Kyeongdeok and Jeong, Chi Yoon},
  journal={IEEE Access}, 
  title={Impact of Device and Environment on Visual-Auditory Sensory Substitution: A Comprehensive Behavioral Analysis Using the vOICe Algorithm}, 
  year={2024},
  volume={12},
  number={},
  pages={90501-90510},
  abstract={Recent studies have revealed that visual-auditory sensory substitution devices (SSDs) can effectively convey visual information to visually impaired or blind individuals through sound. However, SSDs are still not widely available to the visually impaired and blind community. Addressing these challenges requires not only the development of efficient SSD algorithms but also the evaluation of the impact of SSDs on the devices and environments in which they are used. This study represents the first attempt to analyze the impact of the device or environment used for SSDs on users’ perceptual abilities. To achieve this goal, we developed an experimental procedure that involves both the training of the SSD algorithm and the changing environment and devices that receive the audio signal. Two user experiments were conducted and revealed that user perception is significantly affected by the device and environment used for SSDs. These findings underscore the importance of considering the effect of the device and environment in which it will be used when designing an SSD algorithm or training system.},
  keywords={Visualization;Training;Headphones;Deep learning;Wireless sensor networks;Wireless communication;Sensitivity;Sensor systems;Audio-visual systems;Sensory substitution;visual-auditory conversion;visual perception},
  doi={10.1109/ACCESS.2024.3419102},
  ISSN={2169-3536},
  month={June},
  date={2024-06-26}
}

@inproceedings{25-UIST-shin,
author = {Shin, Sungyong and Yi, HyeonBeom and Seo, Junsuk and Jeong, Chi Yoon and Lee, Chang Hee and Lee, Woohun and Nam, Juhan},
title = {Haptic Music Feedback through Audio Decomposition},
year = {2025},
isbn = {9798400720369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746058.3758996},
doi = {10.1145/3746058.3758996},
abstract = {This study presents a novel approach to improving musical haptic wearables by applying deep learning models for music source separation and pitch estimation. The system consists of a haptic vest and a pair of haptic gloves, designed to spatially convey musical elements throughout the body. By isolating instruments from an audio file and recognizing their pitches, our system provides intuitive haptic feedback that discretely represents each instrument’s performance. Specifically, we map the piano to the gloves, the bass to the back of the vest, and the drums to the front of the vest. Then, we conducted a comparative study with a conventional audio-to-haptic method. The results showed that users experienced improved clarity, intuitiveness, and comprehension, indicating enhanced understanding of musical structure.},
booktitle = {Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {41},
numpages = {3},
keywords = {musical haptics, wearable, audio to haptic},
location = {
},
series = {UIST Adjunct '25}
}

@inproceedings{25-VRST-kim,
author = {Kim, Myung Jin (MJ) and Kim, Mooseop and Yi, HyeonBeom and Jeong, Chi Yoon},
title = {Seeing With Sound in Safe Virtual Environments: A Walk-In-Place VR Training System for Users With Visual Impairment Using the vOICe Algorithm},
year = {2025},
isbn = {9798400721182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756884.3770536},
doi = {10.1145/3756884.3770536},
abstract = {We present a virtual reality (VR) training system that supports safe mobility skill development for low-vision users through visual-to-auditory sensory substitution. The system combines the vOICe algorithm with walk-in-place locomotion to enable navigation in immersive environments while minimizing physical risks and spatial requirements. Training with the system follows a two-phase structure: an initial learning phase to build familiarity with visual-to-audio substitution, followed by a navigation phase in which users apply auditory cues to explore and reach destinations in VR. The system provides a safe, controlled environment for developing non-visual spatial awareness and serves as an early exploration of a platform for evaluating sensory substitution techniques. Through this work, we aim to contribute to solutions that promote greater independence in mobility for visually impaired users.},
booktitle = {Proceedings of the 2025 31st ACM Symposium on Virtual Reality Software and Technology},
articleno = {102},
numpages = {2},
keywords = {Virtual Reality, Assistive Technology, Sensory Substitution, Orientation \& Mobility, Walk-in-Place Locomotion},
location = {
},
series = {VRST '25}
}

